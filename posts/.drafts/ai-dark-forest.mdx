---
title: "The Expanding Dark Forest and Generative AI"
description: "How generative AI tools will flood the web with noise and humans will learn to adapt with weird signalling"
updated: "2022-12-10"
startDate: "2022-12-09"
type: "note"
topics: ["Anthropology", "The Web"]
growthStage: "seedling"
---

<AssumedAudience>
  People who have heard of GPT-3 and are vaguely following the advances in
  machine learning, large language models, and image generators.
</AssumedAudience>

<Spacer size="xs" />

<IntroParagraph>

The [[dark forest theory]] of the web points to the increasingly like-like but life-less state of being online.<Footnote idName={1}>[Dark Forest](https://onezero.medium.com/the-dark-forest-theory-of-the-internet-7dc3e68a7cb1) [Theory of](https://onezero.medium.com/the-dark-forest-theory-of-the-internet-7dc3e68a7cb1) [the Internet](https://onezero.medium.com/the-dark-forest-theory-of-the-internet-7dc3e68a7cb1) by Yancey Strickler</Footnote> Most open and publicly available spaces on the web are overrun with bots, advertisers, trolls, data scrapers, clickbait, keyword-stuffing “content creators,” and algorithmically manipulated junk.

</IntroParagraph>

It's like a dark forest that seems eerily quiet – all the living creatures are hidden beneath the ground or up in trees. If they reveal themselves, they risk being attacked by automated predators. Humans who want to engage in informal, unoptimised, personal interactions have to hide in closed spaces like invite-only Slack channels, Discord groups, and email newsletters. Or make themselves [illegible](https://www.ribbonfarm.com/2010/07/26/a-big-little-idea-called-legibility/) and algorithmically incoherent in public venues.

That dark forest is about to <div style={{transform: "scaleX(1.85)", display: "inline-flex", margin: "0 2.2rem"}}>expand</div>. Large Language Models (LLMs) that can instantly generate coherent swaths of human-like text have just joined the party.

Over the last six months, we've seen a flood of LLM copywriting and content-generation products come out: [Jasper](https://www.jasper.ai/), [Moonbeam](https://www.gomoonbeam.com/), [Copy.ai](https://www.copy.ai/), and [Anyword](https://anyword.com/) are just a few. They're designed to pump out advertising copy, blog posts, emails, social media updates, and marketing pages. And they're _really_ good at it.<Footnote idName={2}>Primarily because [GPT-3](https://en.wikipedia.org/wiki/GPT-3) which powers many of these products was specifically trained on text from the web. It's intimately familiar with the style of language we use online.</Footnote>

These models became competent copywriters much faster than people expected – too fast for us to fully process the implications. Many people had their come-to-Jesus moment a few weeks ago when OpenAI released [ChatGPT](https://openai.com/blog/chatgpt/), a slightly more capable version of GPT-3 with an accessible chat-bot style interface. <Footnote idName={3}>They're calling it GPT-3.5. It's the same model with human [reinforcement learning](https://huggingface.co/blog/rlhf) layered on top.</Footnote> The [collective](https://twitter.com/elonmusk/status/1599128577068650498?s=20&t=MgaSdgsaF0uU1lITCVPghw) [shock](https://twitter.com/volodarik/status/1600854935515844610?s=20&t=MgaSdgsaF0uU1lITCVPghw) and [awe](https://twitter.com/yu_angela/status/1599808692085743616?s=20&t=MgaSdgsaF0uU1lITCVPghw) [reaction](https://twitter.com/levie/status/1599156293050433536?s=20&t=MgaSdgsaF0uU1lITCVPghw) made clear how few people had been tracking the progress of these models.

To complicate matters, language models are not the only mimicry machines gathering speed right now. Image generators like [Midjourney](), [DALL-E](), and [Stable Diffusion]() have been on a year-long sprint. In January they could barely render a human face.

---

## A Generated Web

There's a swirl of optimism around how these models will save us from a suite of boring busywork: writing formal emails, internal memos, technical documentation, marketing copy, cover letters, and even negotiating with insurance agents.

But we'll also need to reckon with the trade-offs of the insta-paragraph and 1-click cover image. They're poised to flood the web with generic, generated content.

We have reached the moment when LLMs can link up with [automation software](https://zapier.com/apps/openai/integrations) and social media schedulers to auto-publish a relentless and impossibly banal stream of LinkedIn posts, “engaging” tweets, Facebook updates, and corporate blog posts.

You thought the first page of Google was bunk before? You haven't seen Google where SEO optimizer bros pump out billions of perfectly coherent, predictable, and generic informational articles for every longtail keyword combination under the sun.

It won't just be a flood of text. By threading together text generation, image generation, and various ML techniques, we'll have insta-YouTube video essays, TikToks, Instagram stories.

We've been lamenting the ratio of noise to signal for decades, but

Goodbye to finding original human insights under that pile of cruft.

In a repulsively evocative metaphor, this is the “human centipede epistemology.”

---

A large chunk of the web is about to become a pure bot interchange. They'll write, read, and respond to one another. Mimicking the human attention and interaction that earns advertising $$$. One giant echo chamber of automated interactions.

Tim Hwang's exposé of online advertising in [**Subprime Attention Crisis:**]() **Advertising and the Time Bomb at the Heart of the Internet** points at many of the trends we'd expect to expand.

I buy a lot of Tim Hwang's arguments in . Namely that online advertising is a bubble. The market and algorithmic efficiencies of programmatic advertising mean that ad slots are bought and sold every few milliseconds, faster than a human can blink. Humans actors cannot supervise and

People buying ads on the internet are buying small slots of human attention. Attention that we measure through .

Click farms are warehouses full of humans mimicking sincere human attention; clicking on ads

Bots create the ads, buy and sell the ads, and mimic a real human clicking and reading the ads. It's a facsimile of human attention exchange.

## Passing the Reverse Turing Test

On a web overrun with automated content, our new challenge as little snowflake humans will be to prove we aren't language models. It's the reverse [turing test](https://en.wikipedia.org/wiki/Turing_test).

Before you continue, pause and consider: How would you prove you're not a language model generating predictive text?

<TweetEmbed tweetId="1601541418153304064" />

The oversimplified answer is obvious: do things models aren't capable of.

We should remember that language models, despite how capable they may seem, are at their core a simple linguistic prediction system. They cannot reason like humans reason. They do not have beliefs based on evidence, claims, and principles. They cannot consult external sources and run experiments against objective reality. They do not have embodied experiences, and cannot sense the world as we can sense it; they don't have vision, sound, taste, or touch. They cannot feel emotion or tightly hold a coherent set of values. They are not part of cultures, communities, or histories.

"Humans are members of a community of language-users inhabiting a shared world, and this primal fact makes them essentially different to large language models. Human language users can consult the world to settle their disagreements and update their beliefs. They can, so to speak, “triangulate” on objective reality"

We've designed a system that pattern matches across existing data. It automates a standardised way of writing. We have codified _la langue_ – the official language of the culture. The linguist Saussure

ChatGPT spits out text that sounds like a B+ college essay. Coherent, seemingly comprehensive, but not truly insighful or original.

But there is still _la parle_ – the individual and nuanced speech acts of people.

We have to prove we're not language models with more original thought. More analysis of the world offline of things bots could never access.

Forgive the optimism, but I think we're going to get quite good at it.

It's a good opportunity to do what we're best at – cultural adaptation.

In short answer, we're going to need to get weird. Post-post-post-modernism. Dadaism.

We'll develop neologisms, inside signals, codes, novel dialects. Anything to signal we are not a language model. Not unlike teenagers trying to subvert their elders through in-group jargon, or oppressed communities developing dialects that allow them to safely communicate.

For now, they can't make original, sweeping conclusions from a broad reading of the literature.

There's a lot of hope in this vision. In a world of automated intelligence, our goalposts for intelligence will shift. We'll raise our quality bar for what we expect from humans. When a machine can pump out a great summary of existing work, there's no value in a person doing it.

incorporate unique turns of phrase or personal anecdotes that are specific to their own experiences and backgrounds. They could also include references to current events or other real-world phenomena that would be difficult for an LLM to accurately model or reproduce. Referencing recent events they wouldn't have access to in their trainding data. <Footnote idName={4}>Which feels eerily like a hostage holding up yesterday's newspaper to prove they are actively in danger.</Footnote>

We'll have to become critical thinkers proposing non-probabilistic ideas. fostering a greater sense of critical thinking and skepticism.

We could go down the route of verification and authentication. “Signing” content with (oh god no) the blockchain.
