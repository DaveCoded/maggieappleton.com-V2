---
title: "Algorithmic Transparency"
description: "Algorithms that make their reasoning visible"
type: "pattern"
growthStage: "seedling"
startDate: "2021-10-02"
updated: "2021-10-20"
---

<p class="small-caps">Context</p>

The recommendation algorithm has become a major character in our cultural narratives over the last decade. the dark forces that shove Jordan Peterson videos into every YouTube sidebar and nefarious fad-diet-shills into our Instagram feeds.

Recommendation algorithms blackbox their internal logic and decision-making systems. We don't know what kinds of data go in, and have no visibility into why certain decisions come out. Faced with We fetishise the algorithms and call them "magic" to dismiss the need to make them legible to users.

As an audience we have no visibility into what metrics the engineers are optimising for. The engineers themselves don't fully understand how the algorithms choose what to recommend. The lack of transparency makes it difficult to evaluate why we're being shown certain content. It hides the fact some algorithms maximise for qualities like emotional outrage, shock value, and political extremism. We lack the agency to evaluate and change the algorithms serving us content.

<p class="small-caps">The Pattern</p>

When a piece of content is recommended by an automated system, it should include an epistemic disclosure message explaining **why** it was suggested, and what factors went into that decision.

<p class="small-caps">Examples in the wild</p>

YouTube
Pinterest
Twitter
